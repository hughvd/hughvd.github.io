<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Itamar Hagay Pres</title>

  <meta name="author" content="Hugh Van Deventer">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Hugh Flournoy Van Deventer V</name>
                  </p>

                  <!-- Description -->
                  <p>I am an undergraduate at the University of Michigan studying Computer Science with a minor in
                    Mathematics. My primary
                    focus is on making artificial intelligence systems safer, more interpretable, and better
                    aligned with human values. More specifically, I aim to explore how large language models (LLMs) can
                    operate ethically
                    and effectively in diverse and dynamic contexts.

                  </p>
          

                  <p>At Michigan, I am involved with the <a href="https://lit.eecs.umich.edu/">Language and Information Technologies Lab</a>
                  and have worked alongside <a href="https://ajyl.github.io/about">Dr. Andrew Lee</a> and <a
                    href="https://web.eecs.umich.edu/~mihalcea/">Prof. Rada Mihalcea</a> to leverage interpretability to study
                  toxicity and personas in LLMs.
                  </p>

                  <p>
                    The past summer and fall, I interned with the <a href="https://www.kasl.ai/">Krueger AI Safety Lab</a> at the
                    University of Cambridge working alongside <a href="https://davidscottkrueger.com/">Prof. David Krueger</a>, <a
                      href="https://ekdeepslubana.github.io/">Dr. Ekdeep Singh Lubana</a>, and <a
                      href="https://x.com/LauraRuis">Laura Ruis</a> to develop new inference-time methods for model
                    behavioral control.
                    
                  </p>

                  <p>
                    Most recently, I have also been working with <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a> at
                    the <a href="https://cbs.fas.harvard.edu/">Harvard Center for Brain Science</a> to mechanistically study in-context
                    learning.
                  </p>


                  <p>
                    I first started doing interpretability research with <a href="https://www.neelnanda.io/about">Neel Nanda</a> through the <a
                      href="https://www.matsprogram.org/">ML Alignment & Theory Scholars</a> Program.
                  </p>


                  <!-- Stuff -->
                  <p style="text-align:center">
                    <a href="mailto:hughv@umich.edu">Email</a> &nbsp/&nbsp
                    <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/hughvandeventer/">LinkedIn</a>
                  </p>
                </td>

                <!-- Front image -->
                <td style="padding:2.5%;width:40%;max-width:40%;text-align: left">
                  <a href="images/headshot.jpg"><img style="width:100%;max-width:100%;display: block" alt="profile photo"
                      src="images/headshot.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- Publications -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications (* denotes equal contribution)</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/comp.png" alt="Comp Dynamics" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2412.01003">
                    <papertitle>Competition Dynamics Shape Algorithmic Phases of In-Context Learning</papertitle>
                  </a>
                  <br>
                  <a>Core Francisco Park</a>,
                  <a>Ekdeep Singh Lubana</a>,
                  <strong>Itamar Pres</strong>,
                  and
                  <a>Hidenori Tanaka</a>
                  <br>
                  <em>Preprint (Under Review at ICLR)</em>, 2025
                  <br>
              
              
                  <p> 
                    We introduce a framework for understanding in-context learning (ICL) using a synthetic task based on Markov chain
                    mixtures. We find this task replicates most of the previously described ICL phenomena. We identify four distinct algorithmic phases, blending unigram or bigram statistics with fuzzy retrieval or inference.
                    These phases compete dynamically, revealing sharp transitions in ICL behavior due to changes in training conditions,
                    such as data diversity and context size. I’m proud to have led the interpretability work,
                    quantifying neuron memorization and tracking attention head evolution during training—check it out!
          
                  </p>
                </td>
              </tr>
              


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/benchmark.png" alt="CAA Benchmark" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2410.17245">
                    <papertitle>Towards Reliable Evaluation of Behavior Steering Interventions in LLMs</papertitle>
                  </a>
                  <br>
                  <strong>Itamar Pres</strong>,
                  <a>Laura Ruis</a>,
                  <a>Ekdeep Singh Lubana</a>,
                  and
                  <a>David Krueger</a>
                  
                  <br>
                  <em>NeurIPS workshop on Foundation Model Interventions</em>, 2024 <font color="red"> (Spotlight)</font>
                  <br>
              
              
                  <p>
                  We propose a robust evaluation pipeline for behavioral steering interventions in LLMs, addressing gaps in current
                  methods like subjective metrics and lack of comparability. Our pipeline aligns with downstream tasks, considers model
                  likelihoods, enables cross-behavioral comparisons, and includes baselines. Testing interventions like Contrastive
                  Activation Addition (CAA) and Inference-Time Intervention (ITI), we find their efficacy varies by behavior, with results
                  often overstated and critical distinctions between promoting and suppressing behaviors overlooked.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/toxicity.png" alt="Toxicity DPO" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2401.01967">
                    <papertitle>A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</papertitle>
                  </a>
                  <br>
                  <a>Andrew Lee</a>,
                  <a>Xiaoyan Bai</a>,
                  <strong>Itamar Pres</strong>,
                  <a>Martin Wattenberg</a>,
                  <a>Jonathan K. Kummerfeld</a>,
                  and
                  <a>Rada Mihalcea</a>
                  <br>
                  <em>ICML</em>, 2024 <font color="red"> (Oral)</font>
                  <br>
              
              
                  <p>
                    We study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces
                    toxicity. We first study how toxicity is represented and elicited in pre-trained language
                    models (GPT2-medium, Llama2-7b). We then apply DPO to reduce toxicity and find that capabilities learned from
                    pre-training are not removed,
                    but rather bypassed. We use this insight to demonstrate a simple method to un-align the models,
                    reverting them back to their toxic behavior.
              
                  </p>
                </td>
              </tr>


          



              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        Website template source available <a
                          href="https://github.com/jonbarron/jonbarron_website">here</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>