<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Itamar Hagay Pres</title>
  
  <meta name="author" content="Itamar Hagay Pres">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Itamar Hagay Pres</name>
              </p>

        <!-- Description -->
              <p>I am a undergraduate at the University of Michigan as part of the CBS-NTT Program on Physics of Intelligence, primarily advised by <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a> and <a href="https://davidscottkrueger.com">David Krueger</a>. I did my PhD co-affiliated with EECS, University of Michigan and CBS, Harvard, and was advised by <a href="http://robertdick.org/">Robert Dick</a> and <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>.
              </p>
              <p>
                I am generally interested in designing (faithful) abstractions of phenomena relevant to controlling or aligning neural networks. I am also very interested in better understanding training dynamics of neural networks, especially via a statistical physics perspective. 
              </p>
              <p>
                I graduated with a Bachelor's degree in ECE from Indian Institute of Technology (IIT), Roorkee in 2019. My research in undergraduate was primarily focused on embedded systems, such as energy-efficient machine vision systems.
              </p>

        <!-- Stuff -->
              <p style="text-align:center">
                <a href="mailto:presi@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=FuETqOUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
              </p>
            </td>

        <!-- Front image -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ekdeep.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ekdeep.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr> <td width="100%" valign="top">
        <heading>News</heading>
        <table width="100%" valign="top" border="0" cellspacing="0" cellpadding="2">

        <tr valign="top"> <td> [09/2024] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2406.19370">hidden capabilities in generative models</a> accepted as a <font color="red">spotlight</font> at NeurIPS, 2024.
        </td> </tr>

          
        <tr valign="top"> <td> [08/2024] </td>  <td> 
        Preprint on <a href="https://arxiv.org/abs/2408.12578">a percolation model of emergent capabilities</a> is on arXiv now. 
        </td> </tr>

          
        <tr valign="top"> <td> [06/2024] </td>  <td> 
        Paper on <a href="data/sftjailbreaks.pdf">identifying how jailbreaks bypass safety mechanisms</a> accepted at NeurIPS 2024.
        </td> </tr>

          
        <tr valign="top"> <td> [11/2023] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2311.12786">mechanistically analyzing effects of fine-tuning</a> accepted to ICLR, 2024. 
        </td> </tr>

          
        <tr valign="top"> <td> [10/2023] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2310.17639">analyzing in-context learning as a subjective randomness task</a> accepted to ICLR, 2024. 
        </td> </tr>

          
        <tr valign="top"> <td> [10/2023] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2310.09336">multiplicative emergence of compositional abilities</a> was accepted to NeurIPS, 2023. 
        </td> </tr>

          
        <tr valign="top"> <td> [04/2023] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2211.08422">a mechanistic understanding of loss landscapes</a> was accepted to ICML, 2023.  
        </td> </tr>

          
        <tr valign="top"> <td> [01/2023] </td>  <td> 
        Our work analyzing <a href="https://arxiv.org/pdf/2210.00638.pdf">loss landscape of self-supervised objectives</a> was accepted to ICLR, 2023.  
        </td> </tr>

          
<!--         <tr valign="top"> <td> [05/2022] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2205.11506">unsupervised federated learning</a> was accepted as a <font color="red">spotlight</font> at ICML, 2022.
        </td> </tr>

 -->
        <tr valign="top"> <td> [10/2021] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2106.05956">dynamics of normalization layers</a> was accepted to NeurIPS, 2021.  
        </td> </tr>

          
          <tr valign="top"> <td> [03/2021] </td>  <td> 
        Our work on <a href="https://openreview.net/forum?id=rumv7QmLUue">theory of pruning</a> was accepted as a <font color="red">spotlight</font> at ICLR, 2021.  
        </td> </tr>

        </table>
        </td>  </tr> </tbody></table>


        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (* denotes equal contribution)</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sae.png" alt="SAEs and Formal Languages" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.11767">
                <papertitle>Analyzing (In)Abilities of SAEs via Formal Languages</papertitle>
              </a>
              <br>
              <a href="https://in.linkedin.com/in/abhinav-menon-833404229">Abhinav Menon</a>,
              <a href="https://www.iiit.ac.in/faculty/manish-shrivastava/">Manish Srivastava</a>,
              <a href="https://davidscottkrueger.com">David Krueger</a>, and
              <strong>Ekdeep Singh Lubana</strong>
              <br>
              <em>NeurIPS workshop on Foundation Model Interventions </em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
<!--               <a href="data/sae.bib">bibtex</a> / <a href="https://arxiv.org/abs/2410.11767">arXiv</a> -->
              <p>We use Formal languages to analyze the limitations of SAEs, finding, similar to prior work in disentangled representation learning, that SAEs find correlational features; explicit biasing is necessary to induce causality.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/shattering.png" alt="Representation Shattering" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.17194">
                <papertitle>Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing</papertitle>
              </a>
              <br>
              <a href="https://kentonishi.com">Kento Nishi</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <strong>Ekdeep Singh Lubana*</strong>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka*</a>
              <br>
              <em>Preprint</em>, 2024
              <br> 
<!--               <a href="https://arxiv.org/abs/2410.17194">arXiv</a> -->
              <p>We instantiate a synthetic knowledge graph domain to study how model editing protocols harm broader capabilities, demonstrating all representational organization of different concepts is destroyed under counterfactul edits.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/conceptdynamics.png" alt="Concept learning" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.08309">
                <papertitle>Dynamics of Concept Learning and Compositional Generalization</papertitle>
              </a>
              <br>
              <a href="https://fftyyy.github.io/">Yongyi Yang</a>,
              <a href="https://cfpark00.github.io/">Core Francisco Park</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="http://weihu.me">Wei Hu</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>Preprint</em>, 2024
              <br> 
<!--               <a href="data/conceptdynamics.bib">bibtex</a> / <a href="https://arxiv.org/abs/2410.08039">arXiv</a> -->
              <p>We create a theoretical abstraction of our prior work on compositional generalization and justify the peculiar learning dynamics observed therein, finding there was in fact a quadruple descent embedded therein!</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/percolation.png" alt="Percolation Model of Emergence" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2408.12578">
                <papertitle>A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://www.bdr.riken.jp/en/research/labs/kawaguchi-k/index.html">Kyogo Kawaguchi*</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>Preprint</em>, 2024
              <br> 
<!--               <a href="data/percolation.bib">bibtex</a> / <a href="https://arxiv.org/abs/2408.12578">arXiv</a> -->
              <p>We implicate rapid acquisition of structures underlying the data generating process as the source of sudden learning of capabilities, and analogize knowledge centric capabilities to the process of graph percolation that undergoes a formal second-order phase transition.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/conceptspace.png" alt="Concept Spaces" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2406.19370">
                <papertitle>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</papertitle>
              </a>
              <br>
              <a href="https://cfpark00.github.io/">Core Francisco Park*</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa*</a>,
              <a href="https://ajyl.github.io/about">Andrew Lee*</a>,
              <strong>Ekdeep Singh Lubana*</strong>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka*</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
<!--               <a href="data/conceptspace.bib">bibtex</a> / <a href="https://arxiv.org/abs/2406.19370">arXiv</a> -->
              <p>We analyze a model's learning dynamics in "concept space" and identify sudden transitions where the model, when latently intervened, demonstrates a capability, even if input prompting does not show said capability.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sftjailbreaks.png" alt="SFT and Jailbreaks" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2407.10264">
                <papertitle>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</papertitle>
              </a>
              <br>
              <a href="https://samyakjain0112.github.io/">Samyak Jain</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://openreview.net/profile?id=~Kemal_Oksuz1">Kemal Oksuz</a>,
              <a href="https://thwjoy.github.io">Tom Joy</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,
              <a href="https://amartya18x.github.io/">Amartya Sanyal</a>, and
              <a href="https://puneetkdokania.github.io">Puneet K. Dokania</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br>
              <em>ICML workshop on Mechanistic Interpretability </em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
<!--               <a href="data/sftjailbreaks.bib">bibtex</a> / <a href="https://arxiv.org/abs/2407.10264">arXiv</a> -->
              <p>We use formal languages as a model system to identify the mechanistic changes induced by safety fine-tuning, and how jailbreaks bypass said mechanisms, verifying our claims on Llama models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/matcomp.png" alt="Structure acquisition" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.22244">
                <papertitle>Abrupt Learning in Transformers: A Case Study on Matrix Completion</papertitle>
              </a>
              <br>
              <a href="https://pulkitgopalani.github.io">Pulkit Gopalani</a>,
              <strong>Ekdeep Singh Lubana</strong>, and
              <a href="http://weihu.me">Wei Hu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br> 
<!--               <a href="data/matcomp.bib">bibtex</a> / <a href="arxiv.org">arXiv (coming soon)</a> -->
              <p>We show the acquisition of structures underlying a data-generating process is the driving cause for abrupt learning in Transformers.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/challenges.png" alt="Challenges in LLMs' assurance" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://llm-safety-challenges.github.io">
                <papertitle>Foundational Challenges in Assuring Alignment and Safety of Large Language Models</papertitle>
              </a>
              <br>
              <a href="https://uzman-anwar.github.io">Usman Anwar</a>,
              <a href="https://asaparov.org">Abulhair Saparov<sup>*</sup></a>,
              <a href="https://javirando.com">Javier Rando<sup>*</sup></a>,
              <a href="https://danielpaleka.com">Daniel Paleka<sup>*</sup></a>,
              <a href="https://www.milesturp.in">Miles Turpin<sup>*</sup></a>,
              <a href="https://peterbhase.github.io">Peter Hase<sup>*</sup></a>, 
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="https://ejenner.com">Erik Jenner<sup>*</sup></a>, 
              <a href="https://stephencasper.com">Stephen Casper<sup>*</sup></a>, 
              <a href="https://www.oliversourbut.net">Oliver Sourbut<sup>*</sup></a>, 
              <a href="https://www.benjaminedelman.com">Benjamin Edelman<sup>*</sup></a>,
              <a href="https://zowiezhang.github.io">Zhaowei Zhang<sup>*</sup></a>,
              <a href="https://www.mario-guenther.com">Mario Gunther<sup>*</sup></a>,
              <a href="https://www.korinek.com">Anton Korinek<sup>*</sup></a>,
              <a href="http://josephorallo.webs.upv.es">Jose Hernandez-Orallo<sup>*</sup></a>, and others
              <br>
              <em>Transactions on Machine Learning Research (TMLR) </em>, 2024
              <br> 
<!--               <a href="data/challenges.bib">bibtex</a> / <a href="https://arxiv.org/abs/2404.09932">arXiv</a> / <a href="https://llm-safety-challenges.github.io">website</a> -->
              <p>We identify and discuss 18 foundational challenges in assuring the alignment and safety of large language models (LLMs) and pose 200+ concrete research questions.</p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/capabilities.png" alt="Explosion of capabilities" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.12997">
                <papertitle>Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks</papertitle>
              </a>
              <br>
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br> 
<!--               <a href="data/capabilities.bib">bibtex</a> / <a href="https://arxiv.org/abs/2311.12997">arXiv</a> -->
              <p>We formalize and define a notion of composition of primitive capabilities learned via autoregressive modeling by a Transformer, showing the model's capabilities can "explode", i.e., combinatorially increase if it can compose.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stepwise.png" alt="Understanding stepwise inference" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2402.07757"> 
                <papertitle>Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model</papertitle>
              </a>
              <br>
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="https://janhula.com">Jan Hula</a>,
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <a href="https://kentonishi.com">Kento Nishi</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka<sup>*</sup></a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br> 
<!--               <a href="data/stepwise.bib">bibtex</a> / <a href="https://arxiv.org/abs/2402.07757">arXiv</a> -->
              <p>We cast stepwise inference methods in LLMs as a graph navigation task, finding a synthetic model is sufficient to explain and identify novel characteristics of such methods.</p>
            </td>
          </tr>
          
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mft.png" alt="Mechanistic fine-tuning" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.12786">
                <papertitle>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</papertitle>
              </a>
              <br>
              <a href="https://samyakjain0112.github.io">Samyak Jain<sup>*</sup></a>,
              <a href="https://robertkirk.github.io">Robert Kirk<sup>*</sup></a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>,
              <a href="https://www.egrefen.com">Edward Grefenstette</a>,
              <a href="https://rockt.github.io">Tim Rocktaschel</a>, and
              <a href="https://www.davidscottkrueger.com">David Krueger</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br> 
<!--               <a href="data/mft.bib">bibtex</a> / <a href="https://arxiv.org/abs/2311.12786">arXiv</a> -->
              <p>We show fine-tuning leads to learning of minimal transformations of a pretrained model's capabilities, like a "wrapper", by using procedural tasks defined using Tracr, PCFGs, and TinyStories.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/coinflipping.png" alt="GPT flips coins" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.17639">
                <papertitle>In-Context Learning Dynamics with Random Binary Sequences</papertitle>
              </a>
              <br>
              <a href="https://psychology.fas.harvard.edu/people/eric-bigelow">Eric J. Bigelow</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <a href="http://www.tomerullman.org">Tomer D. Ullman</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br> 
<!--               <a href="data/coinflipping.bib">bibtex</a> / <a href="https://arxiv.org/abs/2310.17639">arXiv</a> -->
              <p>We analyze different LLMs' abilities to model binary sequences generated via different pseduo-random processes, such as a formal automaton, and find that with scale, LLMs are (almost) able to simulate these processes via mere context conditioning.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fomo.png" alt="GPT flips coins" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=nQp1FURyua">
                <papertitle>FoMo Rewards: Can we cast foundation models as reward functions? </papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://johannbrehmer.github.io">Johann Brehmer</a>,
              <a href="http://pimdehaan.com">Pim de Haan</a>, and
              <a href="https://tacocohen.wordpress.com">Taco Cohen</a>
              <br>
              <em>NeurIPS workshop on Foundation Models for Decision Making</em>
              <br> 
<!--               <a href="data/fomo.bib">bibtex</a> / <a href="https://arxiv.org/abs/2312.03881">arXiv</a> -->
              <p>We propose and analyze a pipeline for re-casting an LLM as a generic reward function that interacts with an LVM to enable embodied AI tasks.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/concepts.png" alt="multiplicative emergence" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.09336">
                <papertitle>Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa<sup>*</sup></a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka<sup>*</sup></a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023
              <br> 
<!--               <a href="data/concepts.bib">bibtex</a> / <a href="https://arxiv.org/abs/2310.09336">arXiv</a> -->
              <p>We analyze compositionality in diffusion models, showing that there is a sudden emergence of this capability if models are allowed sufficient training to learn the relevant primitive capabilities.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmc.png" alt="ssl_landscape" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2211.08422">
                <papertitle>Mechanistic Mode Connectivity</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://psychology.fas.harvard.edu/people/eric-bigelow">Eric J. Bigelow</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://www.davidscottkrueger.com">David Krueger</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2023
              <br> 
<!--               <a href="data/mmc.bib">bibtex</a> / <a href="https://arxiv.org/abs/2211.08422">arXiv</a> / <a href="https://github.com/EkdeepSLubana/MMC"> github </a> -->
              <p>We show models that rely on entirely different mechanisms for making their predictions can exhibit mode connectivity, but generally the ones that are mechanistically similar are linearly connected.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ssl_landscape.png" alt="ssl_landscape" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2210.00638.pdf">
                <papertitle>What Shapes the Landscape of Self-Supervised Learning?</papertitle>
              </a>
              <br>
              <a href="http://cat.phys.s.u-tokyo.ac.jp/~zliu/">Liu Ziyin</a>, 
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://cat.phys.s.u-tokyo.ac.jp/~ueda/E_index.html">Masahito Ueda</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2023
              <br> 
<!--               <a href="data/ssl_landscape.bib">bibtex</a> / <a href="https://arxiv.org/pdf/2210.00638.pdf">arXiv</a>  -->
              <p>We present a highly detailed analysis of the landscape of several self-supervised learning objectives to clarify the role of representational collapse.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/graphssl.png" alt="GraphSSL" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.02810">
                <papertitle>Analyzing Data-Centric Properties for Contrastive Learning on Graphs</papertitle> 
              </a>
              <br>
              <a href="https://pujacomputes.github.io">Puja Trivedi</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://markheimann.github.io">Mark Heimann</a>,
              <a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
              <a href="https://jjthiagarajan.com">Jay Jayaraman Thiagarajan</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
              <br>
<!--               <a href="data/graphssl.bib">bibtex</a> / <a href="https://arxiv.org/abs/2208.02810">arXiv</a> / <a href="https://github.com/pujacomputes/datapropsgraphSSL">github</a> -->
              <p>We propose a theoretical framework that demonstrates limitations of popular graph augmentation strategies for self-supervised learning.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/orchestra.png" alt="Orchestra" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.11506">
                <papertitle>Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering</papertitle> 
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://iantangc.github.io">Chi Ian Tang</a>,
              <a href="https://www.fahim-kawsar.net">Fahim Kawsar</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="http://akhilmathurs.github.io">Akhil Mathur</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2022 <font color="red"> (Spotlight)</font> 
              <br>
<!--               <a href="data/orchestra.bib">bibtex</a> / <a href="https://arxiv.org/abs/2205.11506">arXiv</a> / <a href="https://github.com/akhilmathurs/orchestra">github</a> / <a href="https://slideslive.com/38984080/orchestra-unsupervised-federated-learning-via-globally-consistent-clustering">video</a>  -->
              <p>We propose an unsupervised learning method that exploits client heterogeneity to enable privacy preserving, SOTA performance unsupervised federated learning.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/beyondbn.png" alt="beyondbn" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2106.05956.pdf">
                <papertitle>Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021
              <br>
<!--               <a href="data/beyondbn.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/BeyondBatchNorm">github</a> / <a href="https://arxiv.org/abs/2106.05956">arXiv</a> / <a href="https://slideslive.com/38969102/beyond-batchnorm-towards-a-unified-understanding-of-normalization-in-deep-learning?ref=recommended">video</a>  -->
              <p>We develop a general theory to understand the role of normalization layers in improving training dynamics of a neural network at initialization.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/collas.png" alt="quadreg" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2102.02805.pdf">
                <papertitle>How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://www-personal.umich.edu/~pujat/">Puja Trivedi</a>,
              <a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>Conference on Lifelong Learning Agents (CoLLAs)</em>, 2022
              <br>
<!--               <a href="data/emr.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/QRforgetting">github</a> / <a href="https://arxiv.org/abs/2102.02805">arXiv</a> / <a href="https://www.youtube.com/watch?v=gd5YzEnULHU">video</a>  -->
              <br>
              (Also presented at ICML Workshop on Theory and Foundations of Continual Learning, 2021) 
              <br>
              <p>This work demonstrates how quadratic regularization methods for preventing catastrophic forgetting in deep networks rely on a simple heuristic under-the-hood: Interpolation.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gradflow.png" alt="gradflow" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=rumv7QmLUue">
                <papertitle>A Gradient Flow Framework For Analyzing Network Pruning</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong> and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2021 <font color="red"> (Spotlight)</font> 
              <br>
<!--               <a href="data/iclr.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/flowandprune">github</a> / <a href="https://arxiv.org/abs/2009.11839">arXiv</a> / <a href="https://slideslive.com/38953851/a-gradient-flow-framework-for-analyzing-network-pruning">video</a>  -->
              <p>A unified, theoretically-grounded framework for network pruning that helps justify often used heuristics in the field.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Undergraduate Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/minsip.png" alt="minsip" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1HXnGcctlEI96S1edBIHjS88q5WM9a0h9/view?usp=sharing">
                <papertitle>Minimalistic Image Signal Processing for Deep Learning Applications</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              Vinayak Aggarwal, 
              <a href="https://www.iitr.ac.in/departments/ECE/pages/People+Faculty+Pyari_Mohan_Pradhan.html">Pyari Mohan Pradhan</a>
              <br>
              <em>International Conference on Image Processing (ICIP)</em>, 2019
              <br>
<!--               <a href="data/icip.bib">bibtex</a> / -->
              <p>An image signal processing pipeline that allows use of out-of-the-box deep neural networks on RAW images directly retrieved from image sensors.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/foveation.jpg" alt="Digital Foveation" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1WauNRLMVdzOEtlH4ZfawDAyBQiyV0p3I/view?usp=sharing">
                <papertitle>Digital Foveation: An Energy-Aware Machine Vision Framework</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong> and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>IEEE Transactions on Computer-Aided Design of Integrated Circuits and System (TCAD)</em>, 2018
              <br>
<!--               <a href="data/foveation.bib">bibtex</a> / -->
              <p>An energy-efficient machine vision framework inspired by the concept of Fovea in biological vision. Also see <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Simpson_Intelligent_Scene_Caching_to_Improve_Accuracy_for_Energy-Constrained_Embedded_Vision_CVPRW_2020_paper.pdf">follow-up</a> work presented at CVPR workshop, 2020.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/snap.png" alt="SNAP" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1JGDjSyoXzOqPSNYTZ4aQjZJQjQVkhYpn/view?usp=sharing">
                <papertitle>Snap: Chlorophyll Concentration Calculator Using RAW Images of Leaves</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              Mangesh Gurav, and
              <a href="https://www.ee.iitb.ac.in/web/people/faculty/home/mshojaei">Maryam Shojaei Baghini</a>
              <br>
              <em>IEEE Sensors</em>, 2018;
              <em> Global Winner, Ericsson Innovation Awards</em> 2017
              <br>
<!--               <a href="data/snap.bib">bibtex</a> / <a href="https://www.ericsson.com/en/blog/2017/6/ericsson-innovation-awards-2017-working-together-for-the-future-of-food">news</a> -->
              <p>An efficient imaging system that accurately calculates chlorophyll content in leaves by using RAW images.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template source available <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
